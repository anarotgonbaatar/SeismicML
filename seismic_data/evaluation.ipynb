from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error
import numpy
import matplotlib.pyplot as plot
import seaborn
import pandas
from xgboost import XGBRegressor

# Models
# Linear
linear = LinearRegression()
linear.fit( xtrain, ytrain )

with open('linear.model', 'wb') as f:
	pickle.dump( linear, f )

loaded_model = pickle.load( open('linear.model', 'rb') )
score_linear = loaded_model.score( xtest, ytest )
print("Linear model score:", score_linear )

# Decision Tree
decision_tree = DecisionTreeRegressor( random_state = 42 )
decision_tree.fit( xtrain, ytrain )

with open('decision_tree.model', 'wb') as f:
	pickle.dump( decision_tree, f )

loaded_model = pickle.load( open('decision_tree.model', 'rb') )
score_dt = loaded_model.score( xtest, ytest )
print("Decision tree model score:", score_dt )

# Random Forest
random_forest = RandomForestRegressor( n_estimators = 100, n_jobs = -1, random_state = 42 )
random_forest.fit( xtrain, ytrain )

with open('random_forest.model', 'wb') as f:
	pickle.dump( random_forest, f )

loaded_model = pickle.load( open('random_forest.model', 'rb') )
score_rf = loaded_model.score( xtest, ytest )
print("Random forest model score:", score_rf )

# Gradient Boosting
gradient_regression = HistGradientBoostingRegressor( max_iter = 200, random_state=42 )
gradient_regression.fit( xtrain, ytrain)

with open('gradient_regression.model', 'wb') as f:
	pickle.dump( gradient_regression, f )

loaded_model = pickle.load( open('gradient_regression.model', 'rb') )
score_gb = loaded_model.score( xtest, ytest )
print("Gradient regression model score:", score_gb )

# Bagging
bagging_regression = BaggingRegressor( n_estimators=100, n_jobs = -1, random_state=42 )
bagging_regression.fit( xtrain, ytrain)

with open('bagging_regression.model', 'wb') as f:
    pickle.dump ( bagging_regression, f)

loaded_model = pickle.load( open('bagging_regression.model', 'rb') )
score_bag = loaded_model.score( xtest, ytest )
print("Bagging Regression model score:", score_bag )

# XGBoost
xgb = XGBRegressor( n_estimators = 100, learning_rate = 0.1, n_jobs = -1, random_state = 42 )
xgb.fit( xtrain, ytrain )

with open('xgboost.model', 'wb') as f:
    pickle.dump( xgb, f )

loaded_model = pickle.load( open( 'xgboost.model', 'rb' ))
score_xgb = loaded_model.score( xtest, ytest )
print("XGBoost model score:", score_xgb )

# Store predictions for comparison
models = {
    "Linear": score_linear,
    "Decision Tree": score_dt,
    "Random Forest": score_rf,
    "Gradient Boosting": score_gb,
    "Bagging": score_bag,
    "XGBoost": score_xgb
}

metrics = {
    "R²": [],
    "MAE": [],
    "MSE": [],
    "RMSE": [],
    "MAPE": []
}

for name, model in models.items():
    y_pred = model.predict( xtest )
    metrics["R²"].append( r2_score( ytest, y_pred ))
    metrics["MAE"].append( mean_absolute_error( ytest, y_pred ))
    metrics["MSE"].append( mean_squared_error( ytest, y_pred ))
    metrics["RMSE"].append( np.sqrt( mean_squared_error( ytest, y_pred )))
    metrics["MAPE"].append( mean_absolute_percentage_error( ytest, y_pred ))

results_df = pandas.DataFrame( metrics, index=models.keys() )
results_df = results_df.round( 4 )
display( results_df )

# Visualizations
# Heatmap of metrics

# Bar plot of R2 scores

# MAE and RMSE comparison
